{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textstat\n",
    "# !pip install vaderSentiment\n",
    "# !pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from textstat import flesch_reading_ease\n",
    "import nltk\n",
    "from nltk.util import pairwise \n",
    "from vaderSentiment.vaderSentiment import NEGATE, BOOSTER_DICT\n",
    "import re\n",
    "import nltk.data\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Working directory\n",
    "os.chdir('C:/Users/asus/Documents/GitHub/CMSC-197-Miniproject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>Review_Text_str</th>\n",
       "      <th>tag_removed</th>\n",
       "      <th>number_removed</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>useful When least you think so, this product w...</td>\n",
       "      <td>[useful, When, least, you, think, so,, this, p...</td>\n",
       "      <td>useful When least you think so, this product w...</td>\n",
       "      <td>useful When least you think so, this product w...</td>\n",
       "      <td>useful When least you think so, this product w...</td>\n",
       "      <td>[useful, When, least, you, think, so, ,, this,...</td>\n",
       "      <td>[useful, when, least, you, think, so, ,, this,...</td>\n",
       "      <td>[useful, when, least, you, think, so, this, pr...</td>\n",
       "      <td>[useful, least, think, product, save, day, kee...</td>\n",
       "      <td>[[useful, JJ], [when, WRB], [least, JJS], [you...</td>\n",
       "      <td>[[useful, a], [when, n], [least, a], [you, n],...</td>\n",
       "      <td>[useful, when, least, you, think, so, ,, this,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>New era for batteries Lithium batteries are so...</td>\n",
       "      <td>[New, era, for, batteries, Lithium, batteries,...</td>\n",
       "      <td>New era for batteries Lithium batteries are so...</td>\n",
       "      <td>New era for batteries Lithium batteries are so...</td>\n",
       "      <td>New era for batteries Lithium batteries are so...</td>\n",
       "      <td>[New, era, for, batteries, Lithium, batteries,...</td>\n",
       "      <td>[new, era, for, batteries, lithium, batteries,...</td>\n",
       "      <td>[new, era, for, batteries, lithium, batteries,...</td>\n",
       "      <td>[new, era, batteries, lithium, batteries, some...</td>\n",
       "      <td>[[new, JJ], [era, NN], [for, IN], [batteries, ...</td>\n",
       "      <td>[[new, a], [era, n], [for, n], [batteries, n],...</td>\n",
       "      <td>[new, era, for, battery, lithium, battery, be,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>doesn't swing very well. I purchased this swin...</td>\n",
       "      <td>[does not, swing, very, well., I, purchased, t...</td>\n",
       "      <td>does not swing very well. I purchased this swi...</td>\n",
       "      <td>does not swing very well. I purchased this swi...</td>\n",
       "      <td>does not swing very well. I purchased this swi...</td>\n",
       "      <td>[does, not, swing, very, well, ., I, purchased...</td>\n",
       "      <td>[does, not, swing, very, well, ., i, purchased...</td>\n",
       "      <td>[does, not, swing, very, well, i, purchased, t...</td>\n",
       "      <td>[swing, well, purchased, swing, baby, months, ...</td>\n",
       "      <td>[[does, VBZ], [not, RB], [swing, VB], [very, R...</td>\n",
       "      <td>[[does, v], [not, r], [swing, v], [very, r], [...</td>\n",
       "      <td>[do, not, swing, very, well, ., i, purchase, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Great computing! I was looking for an inexpens...</td>\n",
       "      <td>[Great, computing!, I, was, looking, for, an, ...</td>\n",
       "      <td>Great computing! I was looking for an inexpens...</td>\n",
       "      <td>Great computing! I was looking for an inexpens...</td>\n",
       "      <td>Great computing! I was looking for an inexpens...</td>\n",
       "      <td>[Great, computing, !, I, was, looking, for, an...</td>\n",
       "      <td>[great, computing, !, i, was, looking, for, an...</td>\n",
       "      <td>[great, computing, i, was, looking, for, an, i...</td>\n",
       "      <td>[great, computing, looking, inexpensive, desk,...</td>\n",
       "      <td>[[great, JJ], [computing, VBG], [!, .], [i, NN...</td>\n",
       "      <td>[[great, a], [computing, v], [!, n], [i, n], [...</td>\n",
       "      <td>[great, compute, !, i, be, look, for, an, inex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Only use twice a week I only use it twice a we...</td>\n",
       "      <td>[Only, use, twice, a, week, I, only, use, it, ...</td>\n",
       "      <td>Only use twice a week I only use it twice a we...</td>\n",
       "      <td>Only use twice a week I only use it twice a we...</td>\n",
       "      <td>Only use twice a week I only use it twice a we...</td>\n",
       "      <td>[Only, use, twice, a, week, I, only, use, it, ...</td>\n",
       "      <td>[only, use, twice, a, week, i, only, use, it, ...</td>\n",
       "      <td>[only, use, twice, a, week, i, only, use, it, ...</td>\n",
       "      <td>[use, twice, week, use, twice, week, results, ...</td>\n",
       "      <td>[[only, RB], [use, NN], [twice, RB], [a, DT], ...</td>\n",
       "      <td>[[only, r], [use, n], [twice, r], [a, n], [wee...</td>\n",
       "      <td>[only, use, twice, a, week, i, only, use, it, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                        Review_Text  \\\n",
       "0      0  useful When least you think so, this product w...   \n",
       "1      0  New era for batteries Lithium batteries are so...   \n",
       "2      0  doesn't swing very well. I purchased this swin...   \n",
       "3      0  Great computing! I was looking for an inexpens...   \n",
       "4      0  Only use twice a week I only use it twice a we...   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [useful, When, least, you, think, so,, this, p...   \n",
       "1  [New, era, for, batteries, Lithium, batteries,...   \n",
       "2  [does not, swing, very, well., I, purchased, t...   \n",
       "3  [Great, computing!, I, was, looking, for, an, ...   \n",
       "4  [Only, use, twice, a, week, I, only, use, it, ...   \n",
       "\n",
       "                                     Review_Text_str  \\\n",
       "0  useful When least you think so, this product w...   \n",
       "1  New era for batteries Lithium batteries are so...   \n",
       "2  does not swing very well. I purchased this swi...   \n",
       "3  Great computing! I was looking for an inexpens...   \n",
       "4  Only use twice a week I only use it twice a we...   \n",
       "\n",
       "                                         tag_removed  \\\n",
       "0  useful When least you think so, this product w...   \n",
       "1  New era for batteries Lithium batteries are so...   \n",
       "2  does not swing very well. I purchased this swi...   \n",
       "3  Great computing! I was looking for an inexpens...   \n",
       "4  Only use twice a week I only use it twice a we...   \n",
       "\n",
       "                                      number_removed  \\\n",
       "0  useful When least you think so, this product w...   \n",
       "1  New era for batteries Lithium batteries are so...   \n",
       "2  does not swing very well. I purchased this swi...   \n",
       "3  Great computing! I was looking for an inexpens...   \n",
       "4  Only use twice a week I only use it twice a we...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [useful, When, least, you, think, so, ,, this,...   \n",
       "1  [New, era, for, batteries, Lithium, batteries,...   \n",
       "2  [does, not, swing, very, well, ., I, purchased...   \n",
       "3  [Great, computing, !, I, was, looking, for, an...   \n",
       "4  [Only, use, twice, a, week, I, only, use, it, ...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [useful, when, least, you, think, so, ,, this,...   \n",
       "1  [new, era, for, batteries, lithium, batteries,...   \n",
       "2  [does, not, swing, very, well, ., i, purchased...   \n",
       "3  [great, computing, !, i, was, looking, for, an...   \n",
       "4  [only, use, twice, a, week, i, only, use, it, ...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [useful, when, least, you, think, so, this, pr...   \n",
       "1  [new, era, for, batteries, lithium, batteries,...   \n",
       "2  [does, not, swing, very, well, i, purchased, t...   \n",
       "3  [great, computing, i, was, looking, for, an, i...   \n",
       "4  [only, use, twice, a, week, i, only, use, it, ...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [useful, least, think, product, save, day, kee...   \n",
       "1  [new, era, batteries, lithium, batteries, some...   \n",
       "2  [swing, well, purchased, swing, baby, months, ...   \n",
       "3  [great, computing, looking, inexpensive, desk,...   \n",
       "4  [use, twice, week, use, twice, week, results, ...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [[useful, JJ], [when, WRB], [least, JJS], [you...   \n",
       "1  [[new, JJ], [era, NN], [for, IN], [batteries, ...   \n",
       "2  [[does, VBZ], [not, RB], [swing, VB], [very, R...   \n",
       "3  [[great, JJ], [computing, VBG], [!, .], [i, NN...   \n",
       "4  [[only, RB], [use, NN], [twice, RB], [a, DT], ...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0  [[useful, a], [when, n], [least, a], [you, n],...   \n",
       "1  [[new, a], [era, n], [for, n], [batteries, n],...   \n",
       "2  [[does, v], [not, r], [swing, v], [very, r], [...   \n",
       "3  [[great, a], [computing, v], [!, n], [i, n], [...   \n",
       "4  [[only, r], [use, n], [twice, r], [a, n], [wee...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [useful, when, least, you, think, so, ,, this,...  \n",
       "1  [new, era, for, battery, lithium, battery, be,...  \n",
       "2  [do, not, swing, very, well, ., i, purchase, t...  \n",
       "3  [great, compute, !, i, be, look, for, an, inex...  \n",
       "4  [only, use, twice, a, week, i, only, use, it, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load The JSON file\n",
    "with open('data/amazon_data.json', 'r') as f:\n",
    "    reviews = json.load(f)\n",
    "\n",
    "# DataFrame\n",
    "reviews_df = pd.DataFrame(reviews)\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantity\n",
    "result_quantity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['Review_Text']\n",
    "    # Number of words\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    # Number of sentences\n",
    "    sentences = text.split('.')\n",
    "    num_sentences = len(sentences)\n",
    "    # Number of caps\n",
    "    num_caps = sum(1 for c in text if c.isupper())\n",
    "    # Number of punctuation\n",
    "    num_punctuation = sum(text.count(p) for p in string.punctuation)\n",
    "     # Part of speech\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    noun_count = len([word for word, pos in pos_tags if pos in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    verb_count = len([word for word, pos in pos_tags if pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']])\n",
    "    adj_count = len([word for word, pos in pos_tags if pos in ['JJ', 'JJR', 'JJS']])\n",
    "    adv_count = len([word for word, pos in pos_tags if pos in ['RB', 'RBR', 'RBS']])\n",
    "\n",
    "\n",
    "    # Linguistic features results\n",
    "    result_quantity.append({\n",
    "        'Number_of_words': num_words,\n",
    "        'Number_of_sentences': num_sentences,\n",
    "        'Number_of_caps': num_caps,\n",
    "        'Number_of_punctuation': num_punctuation,\n",
    "        'Number_of_nouns': noun_count,\n",
    "        'Number_of_verbs': verb_count,\n",
    "        'Number_of_adjectives': adj_count,\n",
    "        'Number_of_adverbs': adv_count\n",
    "    })\n",
    "\n",
    "# Result dataframe\n",
    "VADER_quantity_df = pd.DataFrame(result_quantity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "VADER_quantity_df.to_json('data/VADER_quantity.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity\n",
    "\n",
    "# Redundancy function\n",
    "def calculate_redundancy(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    # Sum of Levenshtein distances between all pairs of words\n",
    "    total_distance = sum(editdistance.eval(w1, w2) for i, w1 in enumerate(words) for j, w2 in enumerate(words) if i < j)\n",
    "    \n",
    "    # Average Levenshtein distance\n",
    "    n = len(words)\n",
    "    if n > 1:\n",
    "        average_distance = total_distance / (n * (n - 1) / 2)\n",
    "    else:\n",
    "        average_distance = 0\n",
    "    \n",
    "    # Return the redundancy\n",
    "    return 1 - average_distance / len(max(words, key=len))\n",
    "\n",
    "results_complexity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['Review_Text']\n",
    "    # Number of words\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    # Number of sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    # Average word length\n",
    "    total_word_length = sum(len(word) for word in words)\n",
    "    avg_word_length = total_word_length / num_words\n",
    "    # Average sentence length\n",
    "    total_sentence_length = sum(len(sent) for sent in sentences)\n",
    "    avg_sentence_length = total_sentence_length / num_sentences\n",
    "    # Redundance score\n",
    "    redundancy = calculate_redundancy(text)\n",
    "    # Readability score\n",
    "    readability_score = flesch_reading_ease(text)\n",
    "\n",
    "    # linguistic features results\n",
    "    results_complexity.append({\n",
    "        'Average_word_length': avg_word_length,\n",
    "        'Average_sentence_length': avg_sentence_length,\n",
    "        'Redundancy_score': redundancy,\n",
    "        'Readability_score': readability_score,\n",
    "})\n",
    "\n",
    "# Result dataframe\n",
    "VADER_complexity_df = pd.DataFrame(results_complexity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "VADER_complexity_df.to_json('data/VADER_complexity.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversity\n",
    "results_diversity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['lemmatized']\n",
    "    words = text\n",
    "    num_words = len(words)\n",
    "    # Lexical diversity\n",
    "    unique_words = set(words)\n",
    "    lexical_diversity = len(unique_words) / num_words\n",
    "\n",
    "    # Linguistic features results\n",
    "    results_diversity.append({\n",
    "        'Lexical_diversity': lexical_diversity\n",
    "    })\n",
    "\n",
    "\n",
    "# Results dataframe\n",
    "VADER_diversity_df = pd.DataFrame(results_diversity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "VADER_diversity_df.to_json('VADER_diversity.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion\n",
    "\n",
    "# VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sentiment score function\n",
    "def get_sentiment_scores(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return scores['compound'], scores['pos'], scores['neg'], scores['neu']\n",
    "\n",
    "# Polarity function\n",
    "def get_polarity_categories(text):\n",
    "    words = text.split()\n",
    "    polarities = {'pos': 0, 'neg': 0, 'neu': 0}\n",
    "    for word in words:\n",
    "        scores = sia.polarity_scores(word)\n",
    "        for key in polarities.keys():\n",
    "            if scores[key] > 0:\n",
    "                polarities[key] += 1\n",
    "    return polarities\n",
    "\n",
    "# Polarity shifters function\n",
    "def count_polarity_shifters(text):\n",
    "    shifters = ['but', 'however', 'although', 'yet', 'nevertheless']\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word.lower() in shifters:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Intensity modifiers function\n",
    "intensity_modifier_words = BOOSTER_DICT\n",
    "def count_intensity_modifiers(text):\n",
    "    modifiers = intensity_modifier_words\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word.lower() in modifiers:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Negations function\n",
    "negation_words = NEGATE\n",
    "def count_negations(text):\n",
    "    negations = negation_words \n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word.lower() in negations:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_emoticons(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    scores = sid.polarity_scores(text)\n",
    "    num_emoticons = len(emoticons)\n",
    "    return num_emoticons\n",
    "\n",
    "# Dataframe\n",
    "columns = ['sentiment_score', 'positive_score', 'negative_score', 'neutral_score',            \n",
    "           'positive_words', 'negative_words', 'neutral_words',           \n",
    "           'polarity_shifters', 'intensity_modifiers', 'negations', 'emoticons']\n",
    "VADER_emotion_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Extract features\n",
    "for index, row in reviews_df.iterrows():\n",
    "    text = row['lemmatized']\n",
    "    text2 = row['Review_Text']\n",
    "    sentiment_score, positive_score, negative_score, neutral_score = get_sentiment_scores(text2)\n",
    "    polarities = get_polarity_categories(text2)\n",
    "    polarity_shifters = count_polarity_shifters(text)\n",
    "    intensity_modifiers = count_intensity_modifiers(text)\n",
    "    negations = count_negations(text)\n",
    "    emoticons = count_emoticons(text2)\n",
    "    row_results = [sentiment_score, positive_score, negative_score, neutral_score, \n",
    "                   polarities['pos'], polarities['neg'], polarities['neu'],\n",
    "                   polarity_shifters, intensity_modifiers, negations, emoticons]\n",
    "    VADER_emotion_df.loc[index] = row_results\n",
    "\n",
    "# Dataframe to Json file\n",
    "VADER_emotion_df.to_json('data/VADER_emotion.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "os.chdir('C:/Users/asus/Documents/GitHub/CMSC-197-Miniproject/data')\n",
    "\n",
    "# Load dataframes\n",
    "VADER_quantity_df = pd.read_json('VADER_quantity.json')\n",
    "VADER_complexity_df = pd.read_json('VADER_complexity.json')\n",
    "VADER_diversity_df = pd.read_json('VADER_diversity.json')\n",
    "VADER_emotion_df = pd.read_json('VADER_emotion.json')\n",
    "\n",
    "# Concatenate dataframes\n",
    "VADER_df = pd.concat([reviews_df[['Label']], VADER_quantity_df, VADER_complexity_df, VADER_diversity_df, VADER_emotion_df], axis=1)\n",
    "\n",
    "# Dataframe to Json file\n",
    "VADER_df.to_json('VADER.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                             0             1\n",
      "Number_of_words count  10500.000000  10500.000000\n",
      "                mean      63.690190     84.211714\n",
      "                std       58.985292    106.423050\n",
      "                min       16.000000     16.000000\n",
      "                25%       35.000000     32.000000\n",
      "...                             ...           ...\n",
      "emoticons       min        0.000000      0.000000\n",
      "                25%        0.000000      0.000000\n",
      "                50%        0.000000      0.000000\n",
      "                75%        0.000000      0.000000\n",
      "                max        4.000000      6.000000\n",
      "\n",
      "[192 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Rename the labels\n",
    "VADER_df['Label'] = VADER_df['Label'].replace({'__label1__': 'fake', '__label2__': 'real'})\n",
    "\n",
    "# Group the data by label and calculate statistics\n",
    "VADER_statistics = VADER_df.groupby('Label').describe().transpose()\n",
    "\n",
    "# Print statistics\n",
    "print(VADER_statistics)\n",
    "\n",
    "VADER_statistics.to_excel('VADER_statistics.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label  Number_of_words  Number_of_sentences  Number_of_caps  \\\n",
      "0      0         0.002822             0.010152        0.001144   \n",
      "1      0         0.020106             0.015228        0.002859   \n",
      "2      0         0.014109             0.030457        0.003431   \n",
      "3      0         0.010582             0.020305        0.004002   \n",
      "4      0         0.019400             0.015228        0.002859   \n",
      "\n",
      "   Number_of_punctuation  Number_of_nouns  Number_of_verbs  \\\n",
      "0               0.005357         0.005908         0.009615   \n",
      "1               0.012500         0.026588         0.025000   \n",
      "2               0.017857         0.013294         0.023077   \n",
      "3               0.008929         0.013294         0.019231   \n",
      "4               0.010714         0.023634         0.019231   \n",
      "\n",
      "   Number_of_adjectives  Number_of_adverbs  Average_word_length  ...  \\\n",
      "0              0.007663           0.009174             0.123009  ...   \n",
      "1              0.038314           0.022936             0.200208  ...   \n",
      "2              0.030651           0.022936             0.092044  ...   \n",
      "3              0.011494           0.018349             0.104161  ...   \n",
      "4              0.022989           0.022936             0.102568  ...   \n",
      "\n",
      "   positive_score  negative_score  neutral_score  positive_words  \\\n",
      "0           0.217           0.000          0.783        0.018519   \n",
      "1           0.137           0.060          0.803        0.018519   \n",
      "2           0.165           0.053          0.782        0.055556   \n",
      "3           0.099           0.000          0.901        0.009259   \n",
      "4           0.116           0.000          0.884        0.027778   \n",
      "\n",
      "   negative_words  neutral_words  polarity_shifters  intensity_modifiers  \\\n",
      "0        0.000000       0.004542           0.000000             0.012987   \n",
      "1        0.014085       0.022296           0.055556             0.012987   \n",
      "2        0.000000       0.014038           0.055556             0.038961   \n",
      "3        0.000000       0.011974           0.000000             0.038961   \n",
      "4        0.000000       0.021057           0.000000             0.012987   \n",
      "\n",
      "   negations  emoticons  \n",
      "0   0.000000        0.0  \n",
      "1   0.040816        0.0  \n",
      "2   0.061224        0.0  \n",
      "3   0.020408        0.0  \n",
      "4   0.000000        0.0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select the columns to normalize\n",
    "columns_to_normalize = ['positive_words', 'negative_words', 'neutral_words',\n",
    "                        'polarity_shifters', 'intensity_modifiers', 'negations', 'emoticons',\n",
    "                        'Average_word_length', 'Average_sentence_length', 'Number_of_words', 'Number_of_sentences',\n",
    "                        'Number_of_caps', 'Number_of_punctuation', 'Number_of_nouns',\n",
    "                        'Number_of_verbs', 'Number_of_adjectives', 'Number_of_adverbs']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# Normalize\n",
    "VADER_df[columns_to_normalize] = scaler.fit_transform(VADER_df[columns_to_normalize])\n",
    "\n",
    "# Dataframe to Json file\n",
    "VADER_df.to_json('VADER_df_normalized.json', orient='records')\n",
    "print(VADER_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
